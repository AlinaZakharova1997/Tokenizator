{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic_model_gensim_sklearn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlinaZakharova1997/Tokenizator/blob/master/Topic_model_gensim_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zXPZBY70O6Z"
      },
      "source": [
        "# Тематическое моделирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtpH-0290O67"
      },
      "source": [
        "Задача тематического моделирования состоит в том, чтобы разложить слова, употребляющиеся в корпусе на темы и приписать эти темы каждому документу в корпусе.\n",
        "\n",
        "Помимо анализа тематического наполнения корпуса, тематическое моделирование может использоваться для:\n",
        "\n",
        "1) построения вектроных представлений текстов (числа показывают насколько тема выражена в тексте). Такие представления могут быть лучше обычных (TfidfVectorizer, CountVectorizer), т.к. позволяют находить близкие документы даже если в них нет одинаковых слов (но есть слова одной тематики). Тематические векторы могут использоваться для поиска, рекомендаций или как признаки в классификации. \n",
        "\n",
        "2) приписывания тэгов текстам. Так можно автоматически выделять в текстовых потоках тренды, горячие темы. Тэги можно использовать как фильтр в поисковых системах. Тэги нужно приписывать темам вручную, но это проще чем размечать обучающую выборку, так как слова в тематике сразу подказывают название. \n",
        "\n",
        "3) составления тематических словарей\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DBCIiN10O6_"
      },
      "source": [
        "Кроме обычных текстов тематическое моделирование применяется к многим другим данным: запросам пользователей, сайтам, на которые заходят пользователи, музыке, которую слушают пользователи, покупкам в одной корзине, банковским транзакциям и даже днк. \n",
        "\n",
        "\n",
        "Все подходы к тематическому моделированию так или иначе основнованы на:\n",
        "\n",
        "1) модели мешка слов (т.е. порядок слов в документах не учитывается)  \n",
        "2) независимости документов между собой  (т.е. употребление слова W в тексте D_1 никак не влияет на слова в документе D_2)  \n",
        "3) дистрибутивной гипотезе (слова, употребляющиеся вместе, объединяются в темы)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUzjmpPh0O7G"
      },
      "source": [
        "В этой тетрадке для получения тематических моделей используются LDA из gensim и NMF из sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndVzlc2k0O7K"
      },
      "source": [
        "Про LDA (и в целом тематическое моделирование) можно почитать вот эту статью - https://sysblok.ru/knowhow/kak-ponjat-o-chem-tekst-ne-chitaja-ego/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYYyU66B0O7L",
        "outputId": "7aeccfe2-a048-45a3-b9ff-66c08dbd2910"
      },
      "source": [
        "import gensim\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "#! pip3 install pymorphy2\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "#! pip3 install pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import string\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "morph = MorphAnalyzer()\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs1augZM0O7Q"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVGzm1XO0O7Y"
      },
      "source": [
        "Возьмем 10 тыс статьи с Википедии. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWQ48CzK0O7a"
      },
      "source": [
        "Токенизируем самым простым способом и нормализуем Pymorphy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6kqpi-b0O7b"
      },
      "source": [
        "\n",
        "stops = set(stopwords.words('russian'))\n",
        "also_stops = ['свой', 'это','также','мочь','однако', 'иметь', 'быть', 'стать']\n",
        "# чтобы быстрее нормализовать тексты, создадим словарь всех словоформ\n",
        "# нормазуем каждую 1 раз и положим в словарь\n",
        "# затем пройдем по текстам и сопоставим каждой словоформе её нормальную форму\n",
        "\n",
        "def opt_normalize(texts, top=None):\n",
        "    uniq = Counter()\n",
        "    for text in texts:\n",
        "        uniq.update(text)\n",
        "    clean = []\n",
        "    for word, _ in uniq.most_common(top):\n",
        "        if not re.search('[\\'\\\"#\\—\\»i\\«]+?',word) and not re.search('[a-zA-Z]+?',word) and not re.search('[0-9]+?',word):\n",
        "            clean.append((word,_))\n",
        "    norm_uniq = {word:morph.parse(word)[0].normal_form for word, _ in clean}\n",
        "    norm_texts = []\n",
        "    for text in texts:\n",
        "        norm_words = [norm_uniq.get(word) for word in text]\n",
        "        norm_words = [word for word in norm_words if word and word not in stops and word not in also_stops]\n",
        "        norm_texts.append(norm_words)\n",
        "        \n",
        "    return norm_texts\n",
        "\n",
        "def tokenize(text):\n",
        "    words = [word.strip(string.punctuation) for word in text.split()]\n",
        "    words = [word for word in words if word]\n",
        "    \n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNCWkCvX0O7c"
      },
      "source": [
        "corpus = open('wiki_data.txt').read().splitlines()[:10000]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLVdtZ5m0O7h"
      },
      "source": [
        "corpus = opt_normalize([tokenize(text.lower()) for text in corpus], 30000)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6gJ6d9l0O7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a6c22a-f25a-4e63-d0f7-54d8fee4a4f8"
      },
      "source": [
        "corpus[:3]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['нижегородский',\n",
              "  'сельский',\n",
              "  'посёлок',\n",
              "  'район',\n",
              "  'нижегородский',\n",
              "  'область',\n",
              "  'входить',\n",
              "  'состав',\n",
              "  'расположить',\n",
              "  'км',\n",
              "  'юг',\n",
              "  'село',\n",
              "  'км',\n",
              "  'запад',\n",
              "  'город',\n",
              "  'право',\n",
              "  'берег',\n",
              "  'река',\n",
              "  'правый',\n",
              "  'приток',\n",
              "  'река',\n",
              "  'сатис',\n",
              "  'окружить',\n",
              "  'смешанный',\n",
              "  'лес',\n",
              "  'соединить',\n",
              "  'дорога',\n",
              "  'посёлок',\n",
              "  'км',\n",
              "  'дорога',\n",
              "  'посёлок',\n",
              "  'сатис',\n",
              "  'км',\n",
              "  'название',\n",
              "  'являться',\n",
              "  'сугубо',\n",
              "  'официальный',\n",
              "  'местный',\n",
              "  'население',\n",
              "  'использовать',\n",
              "  'исключительно',\n",
              "  'название',\n",
              "  'употребляться',\n",
              "  'языковой',\n",
              "  'оборот',\n",
              "  'ранее',\n",
              "  'использовать',\n",
              "  'название',\n",
              "  'год',\n",
              "  'переселенец',\n",
              "  'соседний',\n",
              "  'село',\n",
              "  'аламасовый',\n",
              "  'расположить',\n",
              "  'соответственно',\n",
              "  'км',\n",
              "  'запад',\n",
              "  'посёлок',\n",
              "  'жить',\n",
              "  'рабочий',\n",
              "  'совхоз',\n",
              "  'центр',\n",
              "  'посёлок',\n",
              "  'сатис',\n",
              "  'возле',\n",
              "  'посёлок',\n",
              "  'расположить',\n",
              "  'активно',\n",
              "  'камень',\n",
              "  'настоящий',\n",
              "  'время',\n",
              "  'официально',\n",
              "  'данные',\n",
              "  'год',\n",
              "  'посёлок',\n",
              "  'насчитываться',\n",
              "  'хозяйство',\n",
              "  'житель',\n",
              "  'осуществляться',\n",
              "  'колодец',\n",
              "  'учреждение',\n",
              "  'отсутствовать',\n",
              "  'год',\n",
              "  'посёлок',\n",
              "  'насчитываться',\n",
              "  'хозяйство',\n",
              "  'житель',\n",
              "  'который',\n",
              "  'трудоспособный',\n",
              "  'возраст',\n",
              "  'январь',\n",
              "  'год',\n",
              "  'посёлок',\n",
              "  'иметься',\n",
              "  'хозяйство',\n",
              "  'настоящий',\n",
              "  'время',\n",
              "  'посёлок',\n",
              "  'оставаться',\n",
              "  'получить',\n",
              "  'развитие',\n",
              "  'благодаря',\n",
              "  'близость',\n",
              "  'святой',\n",
              "  'источник',\n",
              "  'расположить',\n",
              "  'казанский',\n",
              "  'км',\n",
              "  'источник',\n",
              "  'святой',\n",
              "  'серафим',\n",
              "  'посёлок',\n",
              "  'расположить',\n",
              "  'год',\n",
              "  'освятить',\n",
              "  'храм',\n",
              "  'честь',\n",
              "  'серафим',\n",
              "  'саровский'],\n",
              " ['фильм',\n",
              "  'немой',\n",
              "  'короткометражный',\n",
              "  'драматический',\n",
              "  'фильм',\n",
              "  'режиссёр',\n",
              "  'фильм',\n",
              "  'снятой',\n",
              "  'роман',\n",
              "  'виктор',\n",
              "  'состояться',\n",
              "  'франция',\n",
              "  'год',\n",
              "  'фильм',\n",
              "  'считаться',\n",
              "  'самый',\n",
              "  'первый',\n",
              "  'фильм',\n",
              "  'роман',\n",
              "  'парижский',\n",
              "  'рассказывать',\n",
              "  'жизнь',\n",
              "  'красавица',\n",
              "  'собор',\n",
              "  'парижский',\n",
              "  'богоматерь'],\n",
              " ['список',\n",
              "  'остров',\n",
              "  'архипелаг',\n",
              "  'рабочий',\n",
              "  'список',\n",
              "  'координация',\n",
              "  'создание',\n",
              "  'остров',\n",
              "  'архипелаг',\n",
              "  'выделить',\n",
              "  'относительно',\n",
              "  'крупный',\n",
              "  'остров',\n",
              "  'весь',\n",
              "  'остров',\n",
              "  'карта',\n",
              "  'лишь',\n",
              "  'остров',\n",
              "  'южный',\n",
              "  'побережье',\n",
              "  'остров',\n",
              "  'остров',\n",
              "  'юг',\n",
              "  'остров']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rw7dpDR0O7q"
      },
      "source": [
        "\n",
        "ph = gensim.models.Phrases(corpus, scoring='npmi', threshold=0.4) # threshold можно подбирать\n",
        "p = gensim.models.phrases.Phraser(ph)\n",
        "ngrammed_texts = p[corpus]\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lBebSnx0O7q"
      },
      "source": [
        "\n",
        "ph = gensim.models.Phrases(corpus, scoring='npmi', threshold=0.4) # threshold можно подбирать\n",
        "p = gensim.models.phrases.Phraser(ph)\n",
        "ngrammed_texts = p[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O8WRRwa0O7r"
      },
      "source": [
        "### Тематическое моделирование в gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVkT36mr0O7s"
      },
      "source": [
        "Для моделей нужно сделать словарь."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie-jF3iE0O7s"
      },
      "source": [
        "#dictinary = gensim.corpora.Dictionary(texts)\n",
        "mydictinary = gensim.corpora.Dictionary(ngrammed_texts)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfX3FrNV0O7u"
      },
      "source": [
        "mydictinary.filter_extremes(no_above=0.4, no_below=40)\n",
        "mydictinary.compactify()"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QF009Rh0O7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e406424-6b75-4eb8-c93d-3b3409366234"
      },
      "source": [
        "print(mydictinary)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(3156 unique tokens: ['активно', 'благодаря', 'возле', 'возраст', 'входить_состав']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP6UaHo1GO-a"
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1xBkpVa0O7w"
      },
      "source": [
        "Преобразуем наши тексты в мешки слов. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwpqGmiX0O7x"
      },
      "source": [
        "corpus = [mydictinary.doc2bow(text) for text in ngrammed_texts]\n",
        "tfidf = gensim.models.TfidfModel(corpus, id2word=mydictinary)\n",
        "corpus = tfidf[corpus]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVA31_QY0O7z"
      },
      "source": [
        "Про параметры можно почитать в документации:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxzhTthw0O74"
      },
      "source": [
        "?gensim.models.LdaMulticore"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqPjcAD50O77"
      },
      "source": [
        "Основные это num_topics, alpha, eta и passes. \n",
        "\n",
        "**num_topics** - это количество тем. Это основной параметр и настраивать его проще всего. Обычно 200 оптимальное значение. Можно поставить поменьше, если тексты не очень разнообразные или хочется уменьшить время обучения.\n",
        "\n",
        "**alpha** и **eta** - параметры, которые влияют на разреженность распределения документы-темы и темы-слова. У alpha есть значения \"asymmetric\" и \"auto\", которые можно попробовать (по умолчанию стоит \"symmetric\", т.е. не разреженное). Eta можно задать каким-то числом или самому сделать изначальное распределение слов по темам. НО настраивать эти параметры сложно и непонятно и вообще лучше надеяться, что по умолчанию все заработает.\n",
        "\n",
        "**passes** - задает количество проходов по данным. Чем больше, тем лучше сойдется модель, но обучаться будет дольше."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp3CaQ_P0O78"
      },
      "source": [
        "lda = gensim.models.LdaMulticore(corpus, 15, id2word=dictinary, passes=15) # если поддерживается многопоточность\n",
        "# lda = gensim.models.LdaModel(200, id2word=dictinary, passes=5)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrhARfOD0O79"
      },
      "source": [
        "Посмотрим на топики."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAYjLVSZ0O79",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec0a8d7-5ec7-4421-b84b-2a12ae309f17"
      },
      "source": [
        "lda.print_topics()\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.063*\"станция\" + 0.019*\"географический\" + 0.017*\"носитель\" + 0.016*\"украина\" + 0.012*\"линия\" + 0.012*\"дания\" + 0.012*\"юрий\" + 0.011*\"лев\" + 0.011*\"парк\" + 0.010*\"венгрия\"'),\n",
              " (1,\n",
              "  '0.048*\"литература\" + 0.019*\"гитарист\" + 0.009*\"производственный\" + 0.009*\"выпускаться\" + 0.007*\"технический\" + 0.005*\"соединить_штат\" + 0.005*\"список\" + 0.003*\"включить\" + 0.003*\"прага\" + 0.003*\"страница\"'),\n",
              " (2,\n",
              "  '0.016*\"фильм\" + 0.013*\"альбом\" + 0.011*\"песня\" + 0.008*\"группа\" + 0.007*\"фамилия\" + 0.006*\"роль\" + 0.006*\"который\" + 0.006*\"музыка\" + 0.006*\"американский\" + 0.005*\"выпустить\"'),\n",
              " (3,\n",
              "  '0.005*\"г\" + 0.004*\"университет\" + 0.004*\"который\" + 0.003*\"сын\" + 0.003*\"церковь\" + 0.003*\"имя\" + 0.003*\"город\" + 0.003*\"школа\" + 0.003*\"член\" + 0.003*\"время\"'),\n",
              " (4,\n",
              "  '0.024*\"команда\" + 0.023*\"клуб\" + 0.021*\"сезон\" + 0.019*\"турнир\" + 0.015*\"матч\" + 0.015*\"чемпионат\" + 0.012*\"чемпионат_мир\" + 0.011*\"сборная\" + 0.010*\"выступать\" + 0.010*\"находиться_емильчинский\"'),\n",
              " (5,\n",
              "  '0.043*\"летний_олимпийский\" + 0.036*\"игра\" + 0.026*\"олимпийский_игра\" + 0.024*\"следующий_раунд\" + 0.021*\"спортсмен\" + 0.019*\"проходить\" + 0.015*\"каждый_заезд\" + 0.014*\"финал\" + 0.014*\"соревнование\" + 0.014*\"принимать_участие\"'),\n",
              " (6,\n",
              "  '0.075*\"индекс_телефонный\" + 0.074*\"человек_почтовый\" + 0.074*\"код_занимать\" + 0.069*\"украина_находиться\" + 0.067*\"площадь_житомирский\" + 0.066*\"год_составлять\" + 0.059*\"житомирский_коатуа\" + 0.057*\"население_перепись\" + 0.056*\"житомирский_село\" + 0.050*\"р-н_тело\"'),\n",
              " (7,\n",
              "  '0.075*\"олимпийский_игра\" + 0.062*\"завоевать_медаль\" + 0.061*\"летний_олимпийский\" + 0.055*\"игра\" + 0.052*\"принимать_участие\" + 0.049*\"зимний_олимпийский\" + 0.042*\"игра_принимать\" + 0.029*\"история_завоевать\" + 0.029*\"сборная_страна\" + 0.028*\"участие_зимний\"'),\n",
              " (8,\n",
              "  '0.052*\"село\" + 0.021*\"посёлок\" + 0.018*\"район\" + 0.018*\"хутор\" + 0.016*\"сельский\" + 0.015*\"сельский_поселение\" + 0.015*\"поселение\" + 0.015*\"река\" + 0.014*\"район_ростовский\" + 0.013*\"ростовский_область\"'),\n",
              " (9,\n",
              "  '0.088*\"пункт\" + 0.057*\"улица\" + 0.028*\"государство\" + 0.022*\"югославия\" + 0.022*\"бывший\" + 0.022*\"ссср\" + 0.015*\"призёр\" + 0.015*\"белоруссия\" + 0.013*\"нидерланды\" + 0.012*\"профессионал\"'),\n",
              " (10,\n",
              "  '0.112*\"находиться_овручский\" + 0.109*\"овручский_р-н\" + 0.047*\"указ_пвс\" + 0.045*\"переименовать_житомирский\" + 0.043*\"усср_село\" + 0.030*\"граф\" + 0.021*\"графство\" + 0.013*\"р-н_тело\" + 0.012*\"король\" + 0.012*\"герцог\"'),\n",
              " (11,\n",
              "  '0.004*\"система\" + 0.004*\"компания\" + 0.004*\"который\" + 0.004*\"армия\" + 0.003*\"война\" + 0.003*\"бой\" + 0.003*\"корабль\" + 0.003*\"использовать\" + 0.003*\"время\" + 0.003*\"использоваться\"'),\n",
              " (12,\n",
              "  '0.063*\"житомирский_область\" + 0.063*\"почтовый_индекс\" + 0.063*\"телефонный_код\" + 0.063*\"район_житомирский\" + 0.063*\"занимать_площадь\" + 0.062*\"составлять_человек\" + 0.060*\"перепись_год\" + 0.057*\"село_украина\" + 0.050*\"коатуа_население\" + 0.045*\"тело\"'),\n",
              " (13,\n",
              "  '0.096*\"список\" + 0.052*\"подсемейство\" + 0.024*\"род\" + 0.024*\"чехия\" + 0.022*\"словакия\" + 0.019*\"входить\" + 0.015*\"состав\" + 0.015*\"швейцария\" + 0.011*\"роды\" + 0.008*\"польша\"'),\n",
              " (14,\n",
              "  '0.011*\"остров\" + 0.010*\"вид\" + 0.010*\"озеро\" + 0.008*\"город\" + 0.008*\"семейство\" + 0.007*\"расположить\" + 0.007*\"река\" + 0.007*\"часть\" + 0.006*\"район\" + 0.006*\"южный\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wr_LH3C0O7-"
      },
      "source": [
        "ТОП-3 нормальных тем по моей версии без тдф:\n",
        "1. (14,\n",
        "  '0.022*\"армия\" + 0.018*\"войско\" + 0.016*\"самолёт\" + 0.016*\"немецкий\" + 0.015*\"город\" + 0.015*\"часть\" + 0.013*\"бой\" + 0.013*\"противник\" + 0.013*\"район\" + 0.009*\"фронт\"')]\n",
        "2.  (2,\n",
        "  '0.076*\"перепись_год\" + 0.071*\"житомирский_область\" + 0.071*\"составлять_человек\" + 0.070*\"занимать_площадь\" + 0.070*\"почтовый_индекс\" + 0.070*\"телефонный_код\" + 0.069*\"район_житомирский\" + 0.055*\"коатуа_население\" + 0.054*\"тело\" + 0.054*\"село_украина\"')\n",
        "3. (13,\n",
        "  '0.014*\"партия\" + 0.013*\"член\" + 0.011*\"г\" + 0.010*\"президент\" + 0.010*\"совет\" + 0.010*\"правительство\" + 0.010*\"право\" + 0.009*\"страна\" + 0.009*\"национальный\" + 0.009*\"организация\"')\n",
        "\n",
        "\n",
        "  ТОП-3 нормальных тем с тдф:\n",
        "  1.(8,\n",
        "  '0.052*\"село\" + 0.021*\"посёлок\" + 0.018*\"район\" + 0.018*\"хутор\" + 0.016*\"сельский\" + 0.015*\"сельский_поселение\" + 0.015*\"поселение\" + 0.015*\"река\" + 0.014*\"район_ростовский\" + 0.013*\"ростовский_область\"')\n",
        "  2. (12,\n",
        "  '0.063*\"житомирский_область\" + 0.063*\"почтовый_индекс\" + 0.063*\"телефонный_код\" + 0.063*\"район_житомирский\" + 0.063*\"занимать_площадь\" + 0.062*\"составлять_человек\" + 0.060*\"перепись_год\" + 0.057*\"село_украина\" + 0.050*\"коатуа_население\" + 0.045*\"тело\"')\n",
        "  3. (5,\n",
        "  '0.043*\"летний_олимпийский\" + 0.036*\"игра\" + 0.026*\"олимпийский_игра\" + 0.024*\"следующий_раунд\" + 0.021*\"спортсмен\" + 0.019*\"проходить\" + 0.015*\"каждый_заезд\" + 0.014*\"финал\" + 0.014*\"соревнование\" + 0.014*\"принимать_участие\"')\n",
        "  \n",
        "\n",
        "Ещё есть штука для визуализации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRmj-sks0O7-"
      },
      "source": [
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2WuN8QS0O8B",
        "scrolled": false
      },
      "source": [
        "pyLDAvis.gensim.prepare(lda, corpus, dictinary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6t1Zpnv0O8B"
      },
      "source": [
        "На графике должно быть как можно меньше пересекающихся кружков (т.е. темы состоят из разных слов), а сами кружки не должны быть огромными (скорее всего такую тему можно разбить на несколько поменьше)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fDTcsJk0O8C"
      },
      "source": [
        "Можно посмотреть метрики."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ2-cH-70O8G"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBHqGJqZ0O8L"
      },
      "source": [
        "Перплексия показывает насколько хороше моделируется корпус. Чем ближе к нулю, тем лучше. Можно использовать, чтобы настраивать количество проходов по корпусу (когда перестало улучшаться, то можно останавливаться)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWgg94Pp0O8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a87f2f-3539-41e0-e373-49e94b573f0f"
      },
      "source": [
        "lda.log_perplexity(corpus[:1000])\n",
        "# без тдф -7.663149147828475"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12.352014566209919"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMW8JiyJ0O8M"
      },
      "source": [
        "Ещё есть когерентность. Она численно оценивает качество тем (проверяется, что темы состоят из разных слов и что в теме есть топ тематических слов). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7CpOfj70O8M"
      },
      "source": [
        "coherence_model_lda = gensim.models.CoherenceModel(model=lda, \n",
        "                                                  texts=corpus, \n",
        "                                                   dictionary=dictinary, coherence='c_v')\n",
        "#print(corpus[:10])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4S6tWbh0O8N"
      },
      "source": [
        "Чем выше, тем лучше."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EHbB6qV0O8N"
      },
      "source": [
        "topics = []\n",
        "for topic_id, topic in lda.show_topics(num_topics=100, formatted=False):\n",
        "    topic = [word for word, _ in topic]\n",
        "    topics.append(topic)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQMhFxjk0O8N"
      },
      "source": [
        "coherence_model_lda = gensim.models.CoherenceModel(topics=topics, \n",
        "                                                   texts=corpus, \n",
        "                                                   dictionary=dictinary, coherence='c_v')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu_wS1MK0O8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa7dc50-e2ee-4069-c33a-afd9134536cc"
      },
      "source": [
        "coherence_model_lda.get_coherence()\n",
        "# без тдф nan"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQYQ0PXG0O8O"
      },
      "source": [
        "Но все эти числа вспомогательны! Главные критерии качества модели: интерпретируемость и понятность тем (т.е. нужно глазами смотреть на каждую тему), а также польза для практической задачи, которую вы пытаетесь решить."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF-pcsAp0O8O"
      },
      "source": [
        "### Разложение матриц в sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh-wmZEM0O8O",
        "outputId": "2cf55ffd-7d35-4a6e-bf8b-d4c1ee37a298"
      },
      "source": [
        "Image(url=\"https://www.researchgate.net/profile/Andrea_Bertozzi/publication/312157184/figure/fig1/AS:448453387001860@1483931027472/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a.png\",\n",
        "     width=800, height=500)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://www.researchgate.net/profile/Andrea_Bertozzi/publication/312157184/figure/fig1/AS:448453387001860@1483931027472/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a.png\" width=\"800\" height=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWd6WkJu0O8Y"
      },
      "source": [
        "NMF - превращает одну матрицу Words * Documents в произведение двух матриц Words * Topics и Topics * Documents (произведение не точно равно изначальной матрице, но достаточно близко - чем больше Topics, тем точнее, но больше тратиться памяти и времени). \n",
        "\n",
        "Таким образом, взяв одну из получившихся матриц, мы получим или тематические представления документов (вторая матрица - документы на темы), либо слова, разложенные по темам (первая матрица - темы на слова).\n",
        "\n",
        "Отличие от LDA в том, что числа в матрицах не будут вероятностями, а просто каким-то положительными числам. Чем больше, тем сильнее слово или документ связан с темой. \n",
        "\n",
        "(Почему и как это все работает, вы можете почитать отдельно. Для практических задач хватит умения запускать все в sklearn) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmRqZt8z0O8c"
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8gKaLdG0O8d"
      },
      "source": [
        "Sklearn принимает на вход строки, поэтому склеим наши списки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX3dyZTQ0O8d"
      },
      "source": [
        "corpus_text = open('wiki_data.txt').read().splitlines()[:10000]\n",
        "texts = opt_normalize([tokenize(text.lower()) for text in corpus_text], 30000)\n",
        "stexts = [' '.join(text) for text in texts]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_vpvtWH0O8d"
      },
      "source": [
        "Сделаем матрицу слова-документы с помощью TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cv0epg00O8e"
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features=1500, min_df=40, max_df=0.4, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(stexts)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPF6T_J30O8e"
      },
      "source": [
        "Разложим её."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVraefHD0O8e"
      },
      "source": [
        "# n_components - главный параметр в NMF, это количество тем. \n",
        "# Если данных много, то увеличения этого параметра сильно увеличивает время обучения\n",
        "model = NMF(n_components=15)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZs1_Zz0O8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0fc60ba-2618-4d19-d349-c63f3688a3d9"
      },
      "source": [
        "model.fit(X)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
              "    n_components=15, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csWKBXGg0O8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60683a8b-19ce-4fb3-870f-b76f172a31e5"
      },
      "source": [
        "model.components_.shape # матрица темы на слова"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 1500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1cXGprG0O8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9be21b-c91b-4cf4-881e-02a21bb699f8"
      },
      "source": [
        "model.transform(X).shape # матрица документы на темы"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y74pv6MI0O8u"
      },
      "source": [
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfGBPA780O8v"
      },
      "source": [
        "Вытащим словарь, по которому мы построили модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4frQoiRd0O8x"
      },
      "source": [
        "feat_names = vectorizer.get_feature_names()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ca_73Zc0O8z"
      },
      "source": [
        "Теперь посмотрим на матрицу темы-слова, отсортируем её по строкам и возьмем топ N слов, сопоставив индексы со словарём"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WL3mHlM0O80",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0809190a-9c1b-44be-e141-15e4ffce843a"
      },
      "source": [
        "top_words = model.components_.argsort()[:,:-5:-1]\n",
        "\n",
        "for i in range(top_words.shape[0]):\n",
        "    words = [feat_names[j] for j in top_words[i]]\n",
        "    print(i, \"--\".join(words))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 житомирский--житомирский область--район житомирский--почтовый индекс\n",
            "1 летний олимпийский--летний--олимпийский--олимпийский игра\n",
            "2 который--время--город--её\n",
            "3 село--км--расстояние км--река\n",
            "4 вид--семейство--подсемейство--длина\n",
            "5 зимний олимпийский--зимний--олимпийский--олимпийский игра\n",
            "6 клуб--чемпионат--матч--команда\n",
            "7 хутор--ростовский--район ростовский--сельский\n",
            "8 альбом--песня--группа--выпустить\n",
            "9 остров--озеро--расположить--метр\n",
            "10 война--армия--советский--военный\n",
            "11 новоград--новоград волынский--волынский--житомирский\n",
            "12 уезд--волость--состав--округ\n",
            "13 населить пункт--населить--пункт--название\n",
            "14 фильм--роль--режиссёр--актёр\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cSm4ija0O81"
      },
      "source": [
        "У разложения есть метрика, показывающая насколько хорошо восстанавливается изначальная матрица. Чем меньше, тем лучше."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSDqcrcT0O82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f1d745-06c2-4b6d-fd09-ac3e528f0a5a"
      },
      "source": [
        "model.reconstruction_err_"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83.6748955262086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPpzMRCQE5F3"
      },
      "source": [
        "ТОП-3 тем по моей версии:\n",
        "\n",
        "1.клуб--чемпионат--матч--команда\n",
        "2.альбом--песня--группа--выпустить\n",
        "3.фильм--роль--режиссёр--актёр"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYT_grAW0O84"
      },
      "source": [
        "Но как и с LDA - главное это польза от модели и человеческая оценка, получаемых тем."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7nUyCl0O89"
      },
      "source": [
        "## Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0gIJjht0O89"
      },
      "source": [
        "Основаная задача - **построить хорошую тематическую модель с интерпретируемыми топиками с помощью LDA в gensim и NMF в sklearn**.\n",
        "\n",
        "\n",
        "1) сделайте нормализацию (если pymorphy2 работает долго используйте mystem или попробуйте установить быструю версию - `pip install pymorphy2[fast]`, можно использовать какой-то другой токенизатор); - done\n",
        "\n",
        "2) добавьте нграммы (в тетрадке есть закомменченая ячейка с Phrases,  можно также попробовать другие способы построить нграммы);- done \n",
        "\n",
        "3) сделайте хороший словарь (отфильтруйте слишком частотные и редкие слова, попробуйте удалить стоп-слова); - done\n",
        "\n",
        "4) постройте несколько LDA моделей (переберите количество тем, можете поменять alpha, passes), если получаются плохие темы, поработайте дополнительно над предобработкой и словарем; - done\n",
        "\n",
        "5) для самой хорошей модели в отдельной ячейке напечатайте 3 хороших (на ваш вкус) темы; - done\n",
        "\n",
        "6) между словарем и обучением модели добавьте tfidf (`tfidf = gensim.models.TfidfModel(corpus, id2word=dictionary); corpus = tfidf[corpus]`); done\n",
        "\n",
        "7) повторите пункт 4 на преобразованном корпусе (подбирайте параметры, ориентируясь на качество, а не на результаты, которые вы получали без tfidf);-done\n",
        "\n",
        "8) в отдельной ячейке сравните лучшую модель без tfidf и лучшую модель с tfidf (приведите несколько тем, которые стали лучше или хуже, или которых раньше вообще не было; можно привести значения перплексии и когерентности для обеих моделей)- done\n",
        "\n",
        "9) проделайте такие же действия для NMF (образец в конце тетрадки), для построения словаря воспользуйтесь возможностями Count или Tfidf Vectorizer (попробуйте другие значение max_features, min_df, max_df, сделайте нграмы через ngram_range, если хватает памяти), попробуйте такие же количества тем - done\n",
        "\n",
        "10) в отдельной ячейки напечатайте темы лучшей NMF модели, сравните их с теми, что получились в LDA.- done\n",
        "\n",
        "Сохраните тетрадку с экспериментами и положите её на гитхаб, ссылку на неё укажите в форме.\n",
        "\n",
        "**Оцениваться будут главным образом пункты 5, 8 и 10. (2, 3, 2 баллов соответственно). Чтобы заработать остальные 3 балла, нужно хотя бы немного изменить мой код на промежуточных этапах (добавить что-то, указать другие параметры и т.д). **\n",
        "\n",
        "\n",
        "Острожнее интерпретируйте полученные результаты. Если один алгоритм сработал хорошо в этом задании - не значит, что он всегда будет хорошо работать, и наоборот."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9nMZ2KZ0O89"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}